# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
# view dataframe
wisc.df
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
# Create diagnosis vector for later
diagnosis <- wisc.df[,1]
diagnosis
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
# Create diagnosis vector for later
diagnosis <- wisc.df[,1]
print(paste('there are',nrow(wisc.data),'in the dataset'))
print(paste('there are',nrow(wisc.data),'observations (patients) in the dataset'))
diagnosis == 'M'
sum(diagnosis == 'M')
print(paste('there are',sum(diagnosis == 'M'),'observations with malignant diagnoses'))
features <- colnames(wisc.data)
features
features[grep("_mean", features)]
length(features[grep("_mean", features)])
print(paste('there are',length(features[grep("_mean", features)]),'features in the data are suffixed with _mean'))
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data )
summary(wisc.pr)
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( scale(wisc.data) )
summary(wisc.pr)
# create a bipolot of the PCA data
biplot(wisc.pr)
View(wisc.pr)
View(wisc.pr)
# Scatter plot observations by components 1 and 2
plot( c(wisc.pr$x) , col = diagnosis ,
xlab = "PC1", ylab = "PC2")
wisc.pr$X
wisc.pr$x
wisc.pr$x$1
wisc.pr$x@1
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, 1:2 ] , col = diagnosis ,
xlab = "PC1", ylab = "PC2")
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, c(1,2) ] , col = diagnosis ,
xlab = "PC1", ylab = "PC2")
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, c(1,2) ] , xlab = "PC1", ylab = "PC2")
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, c(1,2) ], col=diagnosis, xlab = "PC1", ylab = "PC2")
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, c(1,2) ], col=as.factor(diagnosis), xlab = "PC1", ylab = "PC2")
# Scatter plot observations by components 1 and 3
plot( wisc.pr$x[, c(1,3) ], col=as.factor(diagnosis), xlab = "PC1", ylab = "PC2")
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
# Load the ggplot2 package
library(ggplot2)
# Make a scatter plot colored by diagnosis
ggplot(df) +
aes(PC1, PC2, col=diagnosis) +
geom_point()
# Calculate variance of each component
pr.var <- wisc.data
head(pr.var)
# Calculate variance of each component
pr.var <- wisc.pr
head(pr.var)
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
ylim = c(0, 1), type = "o")
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
install.packages("factoextra")
install.packages("factoextra")
## ggplot based graph
library(factoextra)
library(factoextra)
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
## ggplot based graph
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
wisc.pr$rotation[,1]
wisc.pr$rotation['concave.points_mean',1]
wisc.pr$rotation[,1]
'concave.points_mean'
cumsum(wisc.pr)
cumsum(as.double(wisc.pr))
wisc.pr
cumsum(pr.var)
cumsum(pr.var) >= 80
cumsum(pr.var)
pr.var
pve
cumsum(pve)
cumsum(pve) >= 80
cumsum(pve)
cumsum(pve) >= 0.8
which.max(cumsum(pve) >= 0.8)
print(paste('we need',which.max(cumsum(pve) >= 0.8),'PCs to explain 80% of the variance of the data'))
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
data.dist <- dist(data.scaled,method=”euclidean”)
data.dist <- dist(data.scaled,method='euclidean')
?hclust
wisc.hclust <- hclust(data.dist, method='complete')
?abline
plot(wisc.hclust)
abline(4, col="red", lty=2)
abline(h=4, col="red", lty=2)
plot(wisc.hclust)
abline(h=4, col="red", lty=2)
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
plot(wisc.hclust)
abline(h=20, col="red", lty=2)
?cutree
# cutting tree to have 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
# experimenting with cutting tree to have between 2 and 10 clusters
for (x in 2:10) {
wisc.hclust.clusters <- cutree(wisc.hclust,k=x)
table(wisc.hclust.clusters, diagnosis)
}
# experimenting with cutting tree to have between 2 and 10 clusters
for (x in 2:10) {
wisc.hclust.clusters <- cutree(wisc.hclust,k=x)
print(paste(x,table(wisc.hclust.clusters, diagnosis)))
}
# experimenting with cutting tree to have between 2 and 10 clusters
for (x in 2:10) {
wisc.hclust.clusters <- cutree(wisc.hclust,k=x)
return(x,table(wisc.hclust.clusters, diagnosis))
}
# experimenting with cutting tree to have between 2 and 10 clusters
for (x in 2:10) {
wisc.hclust.clusters <- cutree(wisc.hclust,k=x)
print(x,table(wisc.hclust.clusters, diagnosis))
}
# experimenting with cutting tree to have between 2 and 10 clusters
for (x in 2:10) {
wisc.hclust.clusters <- cutree(wisc.hclust,k=x)
print(table(wisc.hclust.clusters, diagnosis))
}
# cutting tree to have 5 clusters
wisc.hclust.clusters <- cutree(wisc.hclust,k=5)
table(wisc.hclust.clusters, diagnosis)
# experimenting with cutting tree to have between 2 and 10 clusters
m <- list("single", "complete", "average", "ward.D2")
for (x in m) {
wisc.hclust.methods <- hclust(data.dist, method=m)
wisc.hclust.methods.clusters <- cutree(wisc.hclust.methods,k=5,method)
plot(wisc.hclust.methods)
print(table(wisc.hclust.methods.clusters, diagnosis))
}
# experimenting with cutting tree to have between 2 and 10 clusters
m <- list("single", "complete", "average")
for (x in m) {
wisc.hclust.methods <- hclust(data.dist, method=m)
wisc.hclust.methods.clusters <- cutree(wisc.hclust.methods,k=5,method)
plot(wisc.hclust.methods)
print(table(wisc.hclust.methods.clusters, diagnosis))
}
m <- list("single", "complete", "average", "ward.D2")
for (x in m) {
wisc.hclust.methods <- hclust(data.dist, method=m)
wisc.hclust.methods.clusters <- cutree(wisc.hclust.methods,k=5,)
plot(wisc.hclust.methods)
print(table(wisc.hclust.methods.clusters, diagnosis))
}
wisc.hclust.single <- hclust(data.dist, method='single')
# experimenting with cutting tree to have between 2 and 10 clusters
m <- list("single", "complete", "average", "ward.D2")
for (x in m) {
wisc.hclust.methods <- hclust(data.dist, method=m)
wisc.hclust.methods.clusters <- cutree(wisc.hclust.methods,k=5)
plot(wisc.hclust.methods)
print(table(wisc.hclust.methods.clusters, diagnosis))
}
wisc.hclust.single.clusters <- cutree(wisc.hclust.single,k=5)
# experimenting with cutting tree to have between 2 and 10 clusters
m <- list("single", "complete", "average", "ward.D2")
for (x in m) {
wisc.hclust.methods <- hclust(data.dist, method=m)
wisc.hclust.methods.clusters <- cutree(wisc.hclust.methods,k=5)
plot(wisc.hclust.methods)
print(table(wisc.hclust.methods.clusters, diagnosis))
}
wisc.hclust.single <- hclust(data.dist, method='single')
wisc.hclust.single.clusters <- cutree(wisc.hclust.single,k=5)
plot(wisc.hclust.single)
table(wisc.hclust.single.clusters, diagnosis)
# testing different hierarchical clustering methods: single
wisc.hclust.single <- hclust(data.dist, method='single')
wisc.hclust.single.clusters <- cutree(wisc.hclust.single,k=5)
plot(wisc.hclust.single)
table(wisc.hclust.single.clusters, diagnosis)
# the dendrogram looks weird and this does a terrible job of separating the diagnoses
# testing different hierarchical clustering methods: complete
wisc.hclust.complete <- hclust(data.dist, method='complete')
wisc.hclust.complete.clusters <- cutree(wisc.hclust.complete,k=5)
plot(wisc.hclust.complete)
table(wisc.hclust.complete.clusters, diagnosis)
# the dendrogram looks weird and this does a terrible job of separating the diagnoses
# testing different hierarchical clustering methods: average
wisc.hclust.average <- hclust(data.dist, method='average')
wisc.hclust.average.clusters <- cutree(wisc.hclust.average,k=5)
plot(wisc.hclust.average)
table(wisc.hclust.average.clusters, diagnosis)
# thoughts
# testing different hierarchical clustering methods: ward.D2
wisc.hclust.ward.D2 <- hclust(data.dist, method='ward.D2')
wisc.hclust.ward.D2.clusters <- cutree(wisc.hclust.ward.D2,k=5)
plot(wisc.hclust.ward.D2)
table(wisc.hclust.ward.D2.clusters, diagnosis)
# dendrogram looks a lot better than single but also does a terrible job of separating diagneses. would need to optimize
# testing different hierarchical clustering methods: ward.D2
wisc.hclust.ward.D2 <- hclust(data.dist, method='ward.D2')
wisc.hclust.ward.D2.clusters <- cutree(wisc.hclust.ward.D2,k=2)
plot(wisc.hclust.ward.D2)
table(wisc.hclust.ward.D2.clusters, diagnosis)
# dendrogram looks a lot better than single but also does a terrible job of separating diagneses. would need to optimize
# testing different hierarchical clustering methods: ward.D2
wisc.hclust.ward.D2 <- hclust(data.dist, method='ward.D2')
wisc.hclust.ward.D2.clusters <- cutree(wisc.hclust.ward.D2,k=3)
plot(wisc.hclust.ward.D2)
table(wisc.hclust.ward.D2.clusters, diagnosis)
# dendrogram looks a lot better than single but also does a terrible job of separating diagneses. would need to optimize
# testing different hierarchical clustering methods: ward.D2
wisc.hclust.ward.D2 <- hclust(data.dist, method='ward.D2')
wisc.hclust.ward.D2.clusters <- cutree(wisc.hclust.ward.D2,k=4)
plot(wisc.hclust.ward.D2)
table(wisc.hclust.ward.D2.clusters, diagnosis)
# dendrogram looks a lot better than single but also does a terrible job of separating diagneses. would need to optimize
# testing different hierarchical clustering methods: ward.D2
wisc.hclust.ward.D2 <- hclust(data.dist, method='ward.D2')
wisc.hclust.ward.D2.clusters <- cutree(wisc.hclust.ward.D2,k=6)
plot(wisc.hclust.ward.D2)
table(wisc.hclust.ward.D2.clusters, diagnosis)
# dendrogram looks a lot better than single but also does a terrible job of separating diagneses. would need to optimize
# testing different hierarchical clustering methods: ward.D2
wisc.hclust.ward.D2 <- hclust(data.dist, method='ward.D2')
wisc.hclust.ward.D2.clusters <- cutree(wisc.hclust.ward.D2,k=2)
plot(wisc.hclust.ward.D2)
table(wisc.hclust.ward.D2.clusters, diagnosis)
# dendrogram looks a lot better than single but also does a terrible job of separating diagneses. would need to optimize
grps <- cutree(wisc.pr.hclust, k=2)
# hierarchical clustering to get 90% variance
wisc.hclust.ward.D2 <- hclust(wisc.pr$x[,1:7], method='ward.D2')
wisc.pr.hclust <- hclust(wisc.pr$x, method='ward.D2')
wisc.pr.hclust <- hclust(wisc.pr$x)
wisc.pr.hclust <- hclust(wisc.pr)
wisc.pr.hclust <- hclust(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(wisc.pr$x[,1:7])
wisc.pr$x[,1:7]
wisc.pr.hclust <- hclust(wisc.pr$x[,1:7])
wisc.pr$x[,1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7])
grps <- cutree(wisc.pr.hclust, k=2)
grps <- cutree(wisc.pr.hclust, k=2)
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7])
grps <- cutree(wisc.pr.hclust, k=2)
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]))
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
# hierarchical clustering to get 90% variance
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]),method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis)
plot(wisc.pr$x[,1:2], col=as.factor(diagnosis))
g <- as.factor(grps)
levels(g)
g <- as.factor(grps)
levels(g)
g <- as.factor(grps)
levels(g)
g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
# Plot using our re-ordered factor
plot(wisc.pr$x[,1:2], col=g)
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(wisc.pr$x[, 1:7], method="ward.D2")
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters,diagnosis)
# project new data onto PCA space
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
new
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
npc[,1]
npc[,2]
# project new data onto PCA space
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
npc[,1]
npc[,2]
wisc.pr
# project new data onto PCA space
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
# Perform PCA on wisc.data by completing the following code
#wisc.pr <- prcomp( scale(wisc.data) )
wisc.pr <- prcomp(wisc.data, scale=T)
summary(wisc.pr)
# project new data onto PCA space
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
